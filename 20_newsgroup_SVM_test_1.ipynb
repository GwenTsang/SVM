{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification 20newsgroup avec SVM  et GridSearch"
      ],
      "metadata": {
        "id": "l9mIT6QgBNBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6T5DoFb9zOW"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "groups = fetch_20newsgroups()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXMadEX_9zOX",
        "outputId": "dfc9b15d-3847-42b2-aa6e-adda7a2ae701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 7532, 7532)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data_train = fetch_20newsgroups(subset='train', random_state=21)\n",
        "train_label = data_train.target\n",
        "data_test = fetch_20newsgroups(subset='test', random_state=21)\n",
        "test_label = data_test.target\n",
        "len(data_train.data), len(data_test.data), len(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qLyOvsW9zOY",
        "outputId": "738ea271-594d-4cb1-a2c1-2506bc730eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(test_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a 20 classes différentes"
      ],
      "metadata": {
        "id": "tUk7IGFioo6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nettoyage du corpus et réduction du bruit dans ce dernier**\n",
        "\n",
        "On filtre les éléments dans le corpus avec 2 conditions :\n",
        "\n",
        "#### **1. Suppression des entités nommées**\n",
        "\n",
        "Les prénoms peuvent introduire du bruit et biaiser l'apprentissage du modèle. Pour retirer les prénoms, on utilise le corpus NLTK qui contient de très nombreux noms en anglais. Ensuite on retire les noms dans la boucle, en utilisant `words not in all_names`.\n",
        "\n",
        "#### **2. Suppression des caractères spéciaux**\n",
        "\n",
        "On élimine la ponctuation (ex: \"!\", \"?\", \".\") et les nombres avec `words.isalpha()`.\n",
        "\n",
        "\n",
        "#### **Normalisation et lemmatisation**\n",
        "\n",
        "Si un mot satisfait ces deux conditions, on applique `words.lower()` qui transforme le mot en minuscules et on réduit le mot à sa forme de base (lemme) avec `WNL.lemmatize(...)` . Cela réduit la dimensionalité du vocabulaire\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s9WEpxda-0Ar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHU_3LJq9zOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07fd32d-1f17-4c02-c6a8-d718718d0196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Open Multilingual Wordnet for lemmatizer\n",
        "\n",
        "all_names = set(names.words())  # Use set for faster lookup\n",
        "WNL = WordNetLemmatizer()\n",
        "\n",
        "def clean(data):\n",
        "    cleaned = defaultdict(list)\n",
        "    count = 0\n",
        "    for group in data:\n",
        "        for words in group.split():\n",
        "            if words.isalpha() and words not in all_names:\n",
        "                cleaned[count].append(WNL.lemmatize(words.lower()))\n",
        "        cleaned[count] = ' '.join(cleaned[count])\n",
        "        count += 1\n",
        "    return list(cleaned.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WfxIxq79zOY",
        "outputId": "927c2ac8-76f6-4d9f-f0e2-f7a2079144bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bouncing lymenet lehigh university the following address are on the lymenet mailing but are rejecting since the list server originally accepted these address i assume these address have since been improperly functioning mail gateway might also be if you are listed here and would still like to remain on the please write to i will remove these address from the list before the next newsletter go a a general please remember to from all your mailing list before your account is this will save the listserv maintainer from many box lehigh university'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_train = clean(data_train.data)\n",
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnjvie4Q9zOZ",
        "outputId": "139518b5-bfd1-4135-a54c-a07ac5d82d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwtz3JTM9zOZ",
        "outputId": "af501931-b15d-4fd4-9b85-348ff15e752e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7532"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x_test = clean(data_test.data)\n",
        "len(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plP8H9sz9zOa",
        "outputId": "91c6588b-f099-4b48-86a0-b036ec86e03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11314, 1000), (7532, 1000))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train = tf.fit_transform(x_train)\n",
        "X_test = tf.transform(x_test)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cet output signifie qu'on a:\n",
        "* 11314 documents dans $X_{train}$\n",
        "* 7532 documents dans $X_{test}$\n",
        "\n",
        "Chaque document est représenté par un vecteur de 1000 features TF-IDF"
      ],
      "metadata": {
        "id": "Uossv6PmANyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM linéaire**\n",
        "\n",
        "\n",
        "On distingue deux grands types de SVM linéaires : le **Hard margin** SVM et le **Soft margin** SVM.\n",
        "\n",
        "### SVM **Hard Margin**\n",
        "\n",
        "Dans le premier, les instances doivent être parfaitement séparables par un hyperplan linéaire.\n",
        "\n",
        "Pour les points $x_i$ dans la classe $y_i = 1$ on impose :\n",
        "$$\\omega^\\top x_i + b \\ge 1$$\n",
        "\n",
        "Pour les points $x_i$ dans la classe $y_i = -1$ on impose :\n",
        "$$\\omega^\\top x_i + b \\le -1$$\n",
        "\n",
        "On constate qu'un élément $x_i$ est bien classifié si $y_i$ et $(\\omega^\\top x_i + b)$ ont le même signe. Ainsi, on peut reformuler ces deux contraintes de manière unifiée :\n",
        "$$\\boxed{\\forall i,\\quad y_i \\cdot (\\omega^\\top x_i + b) \\ge 1}$$\n",
        "\n",
        "\n",
        "Mais cela suppose que les données soient parfaitement linéairement séparables.\n",
        "\n",
        "Si ce n'est pas le cas, alors, quel que soit l’hyperplan choisi, il existera au moins un point $x_i$ pour lequel $y_i \\cdot (\\omega^\\top x_i + b) \\le 0$ c’est-à-dire au moins un point mal classé (ce qui arrive si $y_i$ et $(\\omega^\\top x_i + b)$ ont des signes différents), ou situé sur la frontière de décision (ce qui arrive si $y_i \\cdot (\\omega^\\top x_i + b) = 0$).\n",
        "\n",
        "Formellement, c'est linéairement séparable si\n",
        "\n",
        "$$\\exists (\\omega,b)\\ \\text{tel que}\\ \\forall i,\\ y_i(\\omega^\\top x_i + b) > 0$$\n",
        "\n",
        "\n",
        "et non linéairement séparable si :\n",
        "\n",
        "$$\\forall (\\omega,b),\\ \\exists i\\ \\text{tel que}\\ y_i(\\omega^\\top x_i + b) \\le 0$$\n",
        "\n",
        "### SVM **Soft Margin**\n",
        "\n",
        "C'est ce pourquoi on utilise un **SVM linéaire soft margin** qui fonctionne même si les données ne sont pas parfaitement linéairement séparables en tolérant des écarts et donc des erreurs de classifications éventuelles.\n",
        "\n",
        "De manière générale, dans un SVM, il faut maximiser l'espacement entre les deux marges (qui correspondent aux deux hyperplans $\\{ x \\in \\mathbb{R}^d \\;|\\; \\omega^\\top x + b = 1 \\}$ et $\\{ x \\in \\mathbb{R}^d \\;|\\; \\omega^\\top x + b = -1 \\}$). Mais, avec certaines données non linéairement séparables, il faut trouver un compromis entre la largeur de la marge et les erreurs de classification.\n",
        "\n",
        "Dans un SVM soft margin, on associe à chaque point $x_i$ une quantité $\\xi_i$ qui indique “de combien il manque” pour satisfaire la condition idéale $y_i(\\omega^\\top x_i + b) \\ge 1$. Pour chaque point $x_i$, la quantité $\\xi_i$ est donnée par :\n",
        "\n",
        "$$\\xi_i = \\max(0, 1 - y_i (\\omega^\\top x_i + b))$$\n",
        "\n",
        "Il y a trois cas possibles (bien classé, dans la marge, mal classé) :\n",
        "\n",
        "* Si le point $x_i$ est bien classé :\n",
        "\n",
        "$$\n",
        "y_i (\\omega^\\top x_i + b) \\ge 1\n",
        "\\quad\\Rightarrow\\quad\n",
        "\\big[1 - y_i (\\omega^\\top x_i + b)\\big] \\le 0\n",
        "\\quad\\Rightarrow\\quad\n",
        "\\xi_i = 0\n",
        "$$\n",
        "\n",
        "* Si le point $x_i$ est dans la marge (entre les deux hyperplans) mais correctement classé :\n",
        "\n",
        "$$\n",
        "0 < y_i (\\omega^\\top x_i + b) < 1\n",
        "\\quad\\Rightarrow\\quad\n",
        "0 < \\xi_i = \\big[1 - y_i (\\omega^\\top x_i + b)\\big] < 1\n",
        "$$\n",
        "\n",
        "* Si le point $x_i$ est mal classé :\n",
        "\n",
        "$$\n",
        "y_i (\\omega^\\top x_i + b) < 0\n",
        "\\quad\\Rightarrow\\quad\n",
        "\\xi_i = \\big[1 - y_i (\\omega^\\top x_i + b)\\big] > 1\n",
        "$$\n",
        "\n",
        "Maintenant, on peut pénaliser le modèle à la fois en fonction du nombre de points du mauvais côté (en comptant à la fois les points dans les marges bien classés et ceux qui sont mal classés) et de l'ampleur des violations (plus la position d'un point s'écarte de sa position attendue, plus la pénalité sur la loss sera forte).\n",
        "\n",
        "Pour ce faire, on dispose de l'hyperparamètre $C$ qui gère le compromis entre la largeur de la marge et les erreurs sur l’échantillon.\n",
        "\n",
        "Si $C$ est grand, on cherche à réduire au maximum ces erreurs quitte à avoir une marge plus petite ; si $C$ est petit, on tolère davantage les violations pour favoriser une marge plus large. Dans ce qui suit, on teste 4 valeurs de $C$ avec `GridSearchCV`.\n",
        "\n",
        "\n",
        "### Sources :\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM\n"
      ],
      "metadata": {
        "id": "vZBJ9RFUoMET"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctQb-sGW9zOd"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "svc_lib = SVC(kernel = 'linear')\n",
        "parameters = {'C' : (0.5,1.0,10,100)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHw9SyED9zOe",
        "outputId": "c3a4b8a0-b8ac-4924-c658-465d1878ae25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time :  307.55232987899996\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import timeit\n",
        "\n",
        "grid_search1 =GridSearchCV(svc_lib, parameters, n_jobs = -1, cv = 3)\n",
        "start_time = timeit.default_timer()\n",
        "grid_search1.fit(X_train, train_label)\n",
        "final = timeit.default_timer()-start_time\n",
        "print(\"Execution Time : \",final)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(X_train, train_label)\n",
        "\n",
        "print(\"coef_.shape :\", linear_svc.coef_.shape)\n",
        "print(\"intercept_.shape :\", linear_svc.intercept_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXi6psTbK077",
        "outputId": "8c1a32de-ca9f-4e63-9f5e-a6d45a32b3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coef_.shape : (20, 1000)\n",
            "intercept_.shape : (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMjzYwsF9zOe",
        "outputId": "55bec116-adfc-4f8d-f090-f9112389b59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1.0}\n",
            "0.7218490556805537\n"
          ]
        }
      ],
      "source": [
        "print(grid_search1.best_params_)\n",
        "print(grid_search1.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pieiyq_k9zOf",
        "outputId": "4ee5b31f-20a5-4903-f5e7-0bd54dec5444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6269251194901753\n"
          ]
        }
      ],
      "source": [
        "grid_search_best1 = grid_search1.best_estimator_\n",
        "accur1 = grid_search_best1.score(X_test, test_label)\n",
        "print(accur1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"coef_.shape :\", linear_svc.coef_.shape)\n",
        "print(\"intercept_.shape :\", linear_svc.intercept_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01yaJcHjNHHD",
        "outputId": "8fa366c1-f258-407e-c13c-661175700ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coef_.shape : (20, 1000)\n",
            "intercept_.shape : (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"Nb classes dans y_train :\", len(np.unique(test_label)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RlBcjg6N3Yh",
        "outputId": "566a13cf-dc34-4a55-b304-3dfd6dbc5e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb classes dans y_train : 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"decision_function_shape:\", svc_lib.decision_function_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBJsyfHOONvJ",
        "outputId": "f55eb99c-8f52-46f3-d527-ef2463351393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decision_function_shape: ovr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OvR est l'abréviation de **One-vs-Rest** : on entraîne 1 classifieur par classe (classe $k$ vs toutes les autres) → n_classes hyperplans.\n",
        "\n",
        "Il semble que c'est plus rapide que la méthode OvO **One versus one** qui cherche $\\frac{n(n-1)}{2}$  hyperplans différents, donc ici puisque $n= 20$ cela chercherait $\\frac{20×19}{2}=190$  hyperplans."
      ],
      "metadata": {
        "id": "KcLDf6HqOkjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "svc_lib = SVC(kernel='linear')\n",
        "\n",
        "# 1) On entraîne correctement\n",
        "svc_lib.fit(X_train, train_label)\n",
        "\n",
        "# 2) On inspecte le modèle entraîné\n",
        "print(\"decision_function_shape:\", svc_lib.decision_function_shape)\n",
        "print(\"classes:\", svc_lib.classes_)\n",
        "print(\"n_support per class:\", svc_lib.n_support_)\n",
        "print(\"support_vectors_.shape:\", svc_lib.support_vectors_.shape)\n",
        "print(\"dual_coef_.shape:\", svc_lib.dual_coef_.shape)\n",
        "\n",
        "# coef_ existe seulement pour kernel='linear'\n",
        "if hasattr(svc_lib, \"coef_\"):\n",
        "    print(\"coef_.shape:\", svc_lib.coef_.shape)\n",
        "\n",
        "print(\"intercept_.shape:\", svc_lib.intercept_.shape)\n",
        "\n",
        "# 3) Maintenant seulement on peut appeler decision_function\n",
        "print(\"decision_function(X_test).shape:\", svc_lib.decision_function(X_test).shape)\n",
        "\n",
        "# 4) Nombre théorique de classifieurs One-vs-One\n",
        "n = len(svc_lib.classes_)\n",
        "print(\"n_classes:\", n, \"  theoretical OvO classifiers:\", n*(n-1)//2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wlebj60PjFi",
        "outputId": "063027cc-e254-4bef-a3e5-ba726fcd1dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decision_function_shape: ovr\n",
            "classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "n_support per class: [421 487 467 497 507 470 446 427 420 458 355 310 509 442 407 471 371 306\n",
            " 402 368]\n",
            "support_vectors_.shape: (8541, 1000)\n",
            "dual_coef_.shape: (19, 8541)\n",
            "coef_.shape: (190, 1000)\n",
            "intercept_.shape: (190,)\n",
            "decision_function(X_test).shape: (7532, 20)\n",
            "n_classes: 20   theoretical OvO classifiers: 190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAtUX5Gp9zOf"
      },
      "source": [
        "# Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJP3GX1W9zOg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "import timeit\n",
        "\n",
        "linear_svc = LinearSVC()\n",
        "parameters = {'C': (0.5, 1, 10,100)}\n",
        "\n",
        "grid_search2 =GridSearchCV(linear_svc, parameters, n_jobs = -1, cv = 3)\n",
        "start_time = timeit.default_timer()\n",
        "grid_search2.fit(X_train, train_label)\n",
        "final = timeit.default_timer()-start_time\n",
        "print(\"Execution Time : \",final)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_svc = LinearSVC()\n",
        "linear_svc.fit(X, y)\n",
        "print(linear_svc.coef_.shape)    # (20, 1000) si TF-IDF de taille 1000\n",
        "print(linear_svc.intercept_.shape) # (20,)"
      ],
      "metadata": {
        "id": "DnyNKVCYKnPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GwCj3mD9P756"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cela signifie qu'il y a **20 hyperplans différents**"
      ],
      "metadata": {
        "id": "kTN-kF_FKr6c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsSz3A9N9zOg"
      },
      "outputs": [],
      "source": [
        "print(grid_search2.best_params_)\n",
        "print(grid_search2.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOTuxjG9zOg"
      },
      "outputs": [],
      "source": [
        "grid_search_best2 = grid_search2.best_estimator_\n",
        "accur2 = grid_search_best2.score(X_test, test_label)\n",
        "accur2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77BSlAMv9zOh"
      },
      "source": [
        "# Model Tuning -> Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6SFAYFp9zOi"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([('tf_id', TfidfVectorizer(stop_words = \"english\")), ('svm_im', LinearSVC())])\n",
        "pipeline\n",
        "\n",
        "parameter = {'tf_id__max_features' : (100,1000, 2000, 8000),\n",
        "             'tf_id__max_df' : (0.25, 0.5),\n",
        "             'tf_id__smooth_idf' : (True, False),\n",
        "             'tf_id__sublinear_tf' : (True, False)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On teste toutes les combinaisons d’hyperparamètres définies dans parameter avec une Cross Validation à 3 folds. Par défaut `refit=True`, on réentraine le meilleur modèle (celui avec le meilleur score moyen en CV) sur tout x_train"
      ],
      "metadata": {
        "id": "AFcHsJygBXCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZBDFnjP9zOi"
      },
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, parameter,cv = 3)\n",
        "grid_search.fit(x_train, train_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F80AUL4y9zOi"
      },
      "outputs": [],
      "source": [
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzTHlYlP9zOi"
      },
      "outputs": [],
      "source": [
        "print(grid_search.best_score_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}