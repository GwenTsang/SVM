{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtkw9DZYt99nt3qHXBGXnn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GwenTsang/SVM/blob/main/20newsgroup_hyperparameters_optimizations_with_gpu_assistance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKSYXlCtiaSj",
        "outputId": "9a084d33-e536-4cc0-cf07-1a717a2d0e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sizes: (11314, 7532, 7532)\n",
            "\n",
            "Selecting best WORD LinearSVC (fast holdout)...\n",
            "Best word SVM: VecCfg(ngram_range=(1, 2), min_df=1, max_df=0.85, analyzer='word') C= 0.7196856730011519\n",
            "\n",
            "Selecting best CHAR LinearSVC (fast holdout)...\n",
            "Best char SVM: VecCfg(ngram_range=(3, 5), min_df=1, max_df=0.9, analyzer='char_wb') C= 0.372759372031494\n",
            "\n",
            "Selecting best WORD LogisticRegression (fast holdout, optional GPU)...\n",
            "Best word LR: VecCfg(ngram_range=(1, 1), min_df=1, max_df=0.85, analyzer='word') C= 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation accuracy (X_val):\n",
            "  word SVM (cal): 0.7879858657243817\n",
            "  char SVM (cal): 0.7685512367491166\n",
            "  word LR      : 0.7703180212014135\n",
            "\n",
            "Best ensemble weights (wordSVM, charSVM, wordLR): (3, 2, 4)\n",
            "Validation accuracy (ensemble): 0.7985865724381626\n",
            "\n",
            "Refitting final models on full training set...\n",
            "\n",
            "TEST RESULTS\n",
            "Accuracy : 0.7140201805629315\n",
            "Macro F1 : 0.7023330470079996\n",
            "\n",
            "Classification report:\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.54      0.50      0.52       319\n",
            "           comp.graphics       0.67      0.74      0.70       389\n",
            " comp.os.ms-windows.misc       0.68      0.62      0.65       394\n",
            "comp.sys.ibm.pc.hardware       0.69      0.67      0.68       392\n",
            "   comp.sys.mac.hardware       0.75      0.71      0.73       385\n",
            "          comp.windows.x       0.86      0.72      0.78       395\n",
            "            misc.forsale       0.79      0.79      0.79       390\n",
            "               rec.autos       0.52      0.78      0.63       396\n",
            "         rec.motorcycles       0.78      0.79      0.79       398\n",
            "      rec.sport.baseball       0.85      0.83      0.84       397\n",
            "        rec.sport.hockey       0.90      0.89      0.89       399\n",
            "               sci.crypt       0.86      0.74      0.80       396\n",
            "         sci.electronics       0.64      0.62      0.63       393\n",
            "                 sci.med       0.78      0.82      0.80       396\n",
            "               sci.space       0.73      0.78      0.75       394\n",
            "  soc.religion.christian       0.69      0.80      0.74       398\n",
            "      talk.politics.guns       0.61      0.68      0.64       364\n",
            "   talk.politics.mideast       0.83      0.78      0.80       376\n",
            "      talk.politics.misc       0.57      0.47      0.51       310\n",
            "      talk.religion.misc       0.46      0.31      0.37       251\n",
            "\n",
            "                accuracy                           0.71      7532\n",
            "               macro avg       0.71      0.70      0.70      7532\n",
            "            weighted avg       0.72      0.71      0.71      7532\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "def make_calibrated(estimator, cv=5, method: str = \"sigmoid\", ensemble: Optional[bool] = None):\n",
        "    \"\"\"\n",
        "    Sklearn compatibility wrapper:\n",
        "    - supports estimator/base_estimator rename\n",
        "    - supports ensemble param when available\n",
        "    \"\"\"\n",
        "    kwargs = dict(cv=cv, method=method)\n",
        "    if ensemble is not None:\n",
        "        kwargs[\"ensemble\"] = ensemble\n",
        "\n",
        "    try:\n",
        "        return CalibratedClassifierCV(estimator=estimator, **kwargs)\n",
        "    except TypeError:\n",
        "        # older sklearn\n",
        "        kwargs.pop(\"ensemble\", None)\n",
        "        return CalibratedClassifierCV(base_estimator=estimator, **kwargs)\n",
        "\n",
        "\n",
        "def weighted_vote_proba(probas: Sequence[np.ndarray], weights: Tuple[int, ...]) -> np.ndarray:\n",
        "    w = np.asarray(weights, dtype=np.float32)\n",
        "    w /= w.sum()\n",
        "    # stack -> (n_models, n_samples, n_classes)\n",
        "    P = np.stack(probas, axis=0).astype(np.float32, copy=False)\n",
        "    # weighted sum over models axis\n",
        "    return np.tensordot(w, P, axes=(0, 0))\n",
        "\n",
        "\n",
        "def _maybe_gpu_logreg_proba(\n",
        "    X_train_csr,\n",
        "    y_train: np.ndarray,\n",
        "    X_pred_csr,\n",
        "    *,\n",
        "    C: float,\n",
        "    max_iter: int,\n",
        "    tol: float,\n",
        ") -> Optional[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Optional GPU path using cuML + CuPy.\n",
        "    If anything fails (missing libs or unsupported sparse), returns None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import cupy as cp\n",
        "        import cupyx.scipy.sparse as cpx_sp\n",
        "        from cuml.linear_model import LogisticRegression as cuLR\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        Xg_tr = cpx_sp.csr_matrix(X_train_csr)\n",
        "        yg_tr = cp.asarray(y_train, dtype=cp.int32)\n",
        "        Xg_pr = cpx_sp.csr_matrix(X_pred_csr)\n",
        "\n",
        "        # cuML API is sklearn-like, but exact params vary by version\n",
        "        model = cuLR(\n",
        "            C=float(C),\n",
        "            penalty=\"l2\",\n",
        "            max_iter=int(max_iter),\n",
        "            tol=float(tol),\n",
        "            fit_intercept=True,\n",
        "            verbose=0,\n",
        "        )\n",
        "        model.fit(Xg_tr, yg_tr)\n",
        "        Pg = model.predict_proba(Xg_pr)\n",
        "        return cp.asnumpy(Pg)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class VecCfg:\n",
        "    ngram_range: Tuple[int, int]\n",
        "    min_df: int\n",
        "    max_df: float\n",
        "    analyzer: str  # \"word\" or \"char_wb\"\n",
        "\n",
        "\n",
        "def _build_vectorizer(cfg: VecCfg, *, max_features: Optional[int]) -> TfidfVectorizer:\n",
        "    base = dict(\n",
        "        strip_accents=\"unicode\",\n",
        "        lowercase=True,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32,\n",
        "        max_features=max_features,\n",
        "    )\n",
        "    if cfg.analyzer == \"word\":\n",
        "        base.update(dict(stop_words=\"english\", analyzer=\"word\"))\n",
        "    else:\n",
        "        base.update(dict(analyzer=\"char_wb\"))\n",
        "    return TfidfVectorizer(\n",
        "        **base,\n",
        "        ngram_range=cfg.ngram_range,\n",
        "        min_df=cfg.min_df,\n",
        "        max_df=cfg.max_df,\n",
        "    )\n",
        "\n",
        "\n",
        "def _two_stage_select_linear_svc(\n",
        "    X_fit_text: Sequence[str],\n",
        "    y_fit: np.ndarray,\n",
        "    X_tune_text: Sequence[str],\n",
        "    y_tune: np.ndarray,\n",
        "    *,\n",
        "    cfgs: List[VecCfg],\n",
        "    C_grid: np.ndarray,\n",
        "    top_k_cfg: int,\n",
        "    max_features: Optional[int],\n",
        "    random_state: int,\n",
        ") -> Tuple[VecCfg, float]:\n",
        "    \"\"\"\n",
        "    Fast selection:\n",
        "    1) score each vectorizer cfg with baseline C=1\n",
        "    2) keep top-k cfgs, sweep C on them\n",
        "    \"\"\"\n",
        "    baseline_C = 1.0\n",
        "    scored: List[Tuple[float, VecCfg, Any, Any]] = []\n",
        "\n",
        "    for cfg in cfgs:\n",
        "        vec = _build_vectorizer(cfg, max_features=max_features)\n",
        "        Xf = vec.fit_transform(X_fit_text)\n",
        "        Xt = vec.transform(X_tune_text)\n",
        "\n",
        "        clf = LinearSVC(C=baseline_C, dual=\"auto\", max_iter=2000, random_state=random_state)\n",
        "        clf.fit(Xf, y_fit)\n",
        "        acc = accuracy_score(y_tune, clf.predict(Xt))\n",
        "        scored.append((acc, cfg, Xf, Xt))\n",
        "\n",
        "    scored.sort(key=lambda t: t[0], reverse=True)\n",
        "    top = scored[: max(1, top_k_cfg)]\n",
        "\n",
        "    best_acc = -1.0\n",
        "    best_cfg = top[0][1]\n",
        "    best_C = float(baseline_C)\n",
        "\n",
        "    for _, cfg, Xf, Xt in top:\n",
        "        for C in C_grid:\n",
        "            clf = LinearSVC(C=float(C), dual=\"auto\", max_iter=2000, random_state=random_state)\n",
        "            clf.fit(Xf, y_fit)\n",
        "            acc = accuracy_score(y_tune, clf.predict(Xt))\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_cfg = cfg\n",
        "                best_C = float(C)\n",
        "\n",
        "    return best_cfg, best_C\n",
        "\n",
        "\n",
        "def _two_stage_select_logreg(\n",
        "    X_fit_text: Sequence[str],\n",
        "    y_fit: np.ndarray,\n",
        "    X_tune_text: Sequence[str],\n",
        "    y_tune: np.ndarray,\n",
        "    *,\n",
        "    cfgs: List[VecCfg],\n",
        "    C_grid: np.ndarray,\n",
        "    top_k_cfg: int,\n",
        "    max_features: Optional[int],\n",
        "    random_state: int,\n",
        "    prefer_gpu: bool,\n",
        ") -> Tuple[VecCfg, float]:\n",
        "    baseline_C = 1.0\n",
        "    scored: List[Tuple[float, VecCfg, Any, Any]] = []\n",
        "\n",
        "    for cfg in cfgs:\n",
        "        vec = _build_vectorizer(cfg, max_features=max_features)\n",
        "        Xf = vec.fit_transform(X_fit_text)\n",
        "        Xt = vec.transform(X_tune_text)\n",
        "\n",
        "        # Try GPU first (if asked); otherwise CPU directly\n",
        "        Pg = _maybe_gpu_logreg_proba(Xf, y_fit, Xt, C=baseline_C, max_iter=1500, tol=1e-3) if prefer_gpu else None\n",
        "        if Pg is not None:\n",
        "            pred = Pg.argmax(axis=1)\n",
        "            acc = accuracy_score(y_tune, pred)\n",
        "        else:\n",
        "            clf = LogisticRegression(\n",
        "                solver=\"saga\",\n",
        "                penalty=\"l2\",\n",
        "                C=float(baseline_C),\n",
        "                max_iter=1500,\n",
        "                tol=1e-3,\n",
        "                n_jobs=-1,\n",
        "                multi_class=\"multinomial\",\n",
        "                random_state=random_state,\n",
        "            )\n",
        "            clf.fit(Xf, y_fit)\n",
        "            acc = accuracy_score(y_tune, clf.predict(Xt))\n",
        "\n",
        "        scored.append((acc, cfg, Xf, Xt))\n",
        "\n",
        "    scored.sort(key=lambda t: t[0], reverse=True)\n",
        "    top = scored[: max(1, top_k_cfg)]\n",
        "\n",
        "    best_acc = -1.0\n",
        "    best_cfg = top[0][1]\n",
        "    best_C = float(baseline_C)\n",
        "\n",
        "    for _, cfg, Xf, Xt in top:\n",
        "        for C in C_grid:\n",
        "            Pg = _maybe_gpu_logreg_proba(Xf, y_fit, Xt, C=float(C), max_iter=2000, tol=1e-3) if prefer_gpu else None\n",
        "            if Pg is not None:\n",
        "                pred = Pg.argmax(axis=1)\n",
        "                acc = accuracy_score(y_tune, pred)\n",
        "            else:\n",
        "                clf = LogisticRegression(\n",
        "                    solver=\"saga\",\n",
        "                    penalty=\"l2\",\n",
        "                    C=float(C),\n",
        "                    max_iter=2000,\n",
        "                    tol=1e-3,\n",
        "                    n_jobs=-1,\n",
        "                    multi_class=\"multinomial\",\n",
        "                    random_state=random_state,\n",
        "                )\n",
        "                clf.fit(Xf, y_fit)\n",
        "                acc = accuracy_score(y_tune, clf.predict(Xt))\n",
        "\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_cfg = cfg\n",
        "                best_C = float(C)\n",
        "\n",
        "    return best_cfg, best_C\n",
        "\n",
        "\n",
        "def main(random_state: int = 21, prefer_gpu_for_lr: bool = True) -> None:\n",
        "    # 1) Load dataset\n",
        "    data_train = fetch_20newsgroups(\n",
        "        subset=\"train\",\n",
        "        remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "        random_state=random_state,\n",
        "    )\n",
        "    data_test = fetch_20newsgroups(\n",
        "        subset=\"test\",\n",
        "        remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    X_train, y_train = data_train.data, data_train.target\n",
        "    X_test, y_test = data_test.data, data_test.target\n",
        "\n",
        "    print(\"Sizes:\", (len(X_train), len(X_test), len(y_test)))\n",
        "\n",
        "    # 2) Splits:\n",
        "    #    - X_fit: used to fit candidates\n",
        "    #    - X_tune: used for fast selection of vectorizer/C\n",
        "    #    - then split tune into calibration + validation for ensemble weights\n",
        "    X_fit, X_tune, y_fit, y_tune = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.10,\n",
        "        random_state=random_state,\n",
        "        stratify=y_train,\n",
        "    )\n",
        "    X_cal, X_val, y_cal, y_val = train_test_split(\n",
        "        X_tune,\n",
        "        y_tune,\n",
        "        test_size=0.50,\n",
        "        random_state=random_state,\n",
        "        stratify=y_tune,\n",
        "    )\n",
        "\n",
        "    # 3) Search spaces (kept close to your original, but tuned for speed)\n",
        "    word_cfgs = [\n",
        "        VecCfg(ngram_range=ng, min_df=md, max_df=xd, analyzer=\"word\")\n",
        "        for ng in [(1, 1), (1, 2)]\n",
        "        for md in [1, 2, 5]\n",
        "        for xd in [0.85, 0.95, 1.0]\n",
        "    ]\n",
        "    char_cfgs = [\n",
        "        VecCfg(ngram_range=ng, min_df=md, max_df=xd, analyzer=\"char_wb\")\n",
        "        for ng in [(3, 5), (3, 6), (4, 7)]\n",
        "        for md in [1, 2, 5]\n",
        "        for xd in [0.90, 0.98, 1.0]\n",
        "    ]\n",
        "\n",
        "    # Smaller C grids than your original randomized searches (fewer fits)\n",
        "    C_svm = np.logspace(-1, 1, 8)   # 0.1 .. 10\n",
        "    C_lr = np.logspace(-1, 1, 7)    # 0.1 .. 10\n",
        "\n",
        "    # max_features caps vocab size (big speed win). Increase if you want max accuracy.\n",
        "    max_features_word = 250_000\n",
        "    max_features_char = 400_000\n",
        "\n",
        "    top_k_cfg = 5\n",
        "\n",
        "    print(\"\\nSelecting best WORD LinearSVC (fast holdout)...\")\n",
        "    best_word_cfg, best_word_C = _two_stage_select_linear_svc(\n",
        "        X_fit, y_fit, X_cal, y_cal,\n",
        "        cfgs=word_cfgs, C_grid=C_svm, top_k_cfg=top_k_cfg,\n",
        "        max_features=max_features_word, random_state=random_state\n",
        "    )\n",
        "    print(\"Best word SVM:\", best_word_cfg, \"C=\", best_word_C)\n",
        "\n",
        "    print(\"\\nSelecting best CHAR LinearSVC (fast holdout)...\")\n",
        "    best_char_cfg, best_char_C = _two_stage_select_linear_svc(\n",
        "        X_fit, y_fit, X_cal, y_cal,\n",
        "        cfgs=char_cfgs, C_grid=C_svm, top_k_cfg=top_k_cfg,\n",
        "        max_features=max_features_char, random_state=random_state\n",
        "    )\n",
        "    print(\"Best char SVM:\", best_char_cfg, \"C=\", best_char_C)\n",
        "\n",
        "    print(\"\\nSelecting best WORD LogisticRegression (fast holdout, optional GPU)...\")\n",
        "    best_lr_cfg, best_lr_C = _two_stage_select_logreg(\n",
        "        X_fit, y_fit, X_cal, y_cal,\n",
        "        cfgs=word_cfgs, C_grid=C_lr, top_k_cfg=top_k_cfg,\n",
        "        max_features=max_features_word, random_state=random_state,\n",
        "        prefer_gpu=prefer_gpu_for_lr\n",
        "    )\n",
        "    print(\"Best word LR:\", best_lr_cfg, \"C=\", best_lr_C)\n",
        "\n",
        "    # 4) Fit models on X_fit, calibrate SVMs on X_cal, tune ensemble weights on X_val\n",
        "    # Word SVM\n",
        "    vec_word = _build_vectorizer(best_word_cfg, max_features=max_features_word)\n",
        "    Xf_word = vec_word.fit_transform(X_fit)\n",
        "    Xcal_word = vec_word.transform(X_cal)\n",
        "    Xval_word = vec_word.transform(X_val)\n",
        "\n",
        "    base_word_svm = LinearSVC(C=best_word_C, dual=\"auto\", max_iter=4000, random_state=random_state)\n",
        "    base_word_svm.fit(Xf_word, y_fit)\n",
        "    cal_word_svm = make_calibrated(base_word_svm, cv=\"prefit\", method=\"sigmoid\")\n",
        "    cal_word_svm.fit(Xcal_word, y_cal)\n",
        "\n",
        "    # Char SVM\n",
        "    vec_char = _build_vectorizer(best_char_cfg, max_features=max_features_char)\n",
        "    Xf_char = vec_char.fit_transform(X_fit)\n",
        "    Xcal_char = vec_char.transform(X_cal)\n",
        "    Xval_char = vec_char.transform(X_val)\n",
        "\n",
        "    base_char_svm = LinearSVC(C=best_char_C, dual=\"auto\", max_iter=4000, random_state=random_state)\n",
        "    base_char_svm.fit(Xf_char, y_fit)\n",
        "    cal_char_svm = make_calibrated(base_char_svm, cv=\"prefit\", method=\"sigmoid\")\n",
        "    cal_char_svm.fit(Xcal_char, y_cal)\n",
        "\n",
        "    # Word LR\n",
        "    vec_lr = _build_vectorizer(best_lr_cfg, max_features=max_features_word)\n",
        "    Xf_lr = vec_lr.fit_transform(X_fit)\n",
        "    Xval_lr = vec_lr.transform(X_val)\n",
        "\n",
        "    # Try GPU for final tuning-time LR too (falls back automatically)\n",
        "    P_lr_val = _maybe_gpu_logreg_proba(Xf_lr, y_fit, Xval_lr, C=best_lr_C, max_iter=2500, tol=1e-3) if prefer_gpu_for_lr else None\n",
        "    if P_lr_val is None:\n",
        "        lr = LogisticRegression(\n",
        "            solver=\"saga\",\n",
        "            penalty=\"l2\",\n",
        "            C=best_lr_C,\n",
        "            max_iter=2500,\n",
        "            tol=1e-3,\n",
        "            n_jobs=-1,\n",
        "            multi_class=\"multinomial\",\n",
        "            random_state=random_state,\n",
        "        )\n",
        "        lr.fit(Xf_lr, y_fit)\n",
        "        P_lr_val = lr.predict_proba(Xval_lr)\n",
        "    else:\n",
        "        lr = None  # will refit later on full train\n",
        "\n",
        "    # Validation metrics (on X_val)\n",
        "    val_pred_word = cal_word_svm.predict(Xval_word)\n",
        "    val_pred_char = cal_char_svm.predict(Xval_char)\n",
        "    val_pred_lr = P_lr_val.argmax(axis=1)\n",
        "\n",
        "    print(\"\\nValidation accuracy (X_val):\")\n",
        "    print(\"  word SVM (cal):\", accuracy_score(y_val, val_pred_word))\n",
        "    print(\"  char SVM (cal):\", accuracy_score(y_val, val_pred_char))\n",
        "    print(\"  word LR      :\", accuracy_score(y_val, val_pred_lr))\n",
        "\n",
        "    # 5) Tune ensemble weights on X_val\n",
        "    P_word_val = cal_word_svm.predict_proba(Xval_word)\n",
        "    P_char_val = cal_char_svm.predict_proba(Xval_char)\n",
        "\n",
        "    candidates = [1, 2, 3, 4]\n",
        "    best_w = (1, 1, 1)\n",
        "    best_acc = -1.0\n",
        "    for w in itertools.product(candidates, repeat=3):\n",
        "        P = weighted_vote_proba([P_word_val, P_char_val, P_lr_val], w)\n",
        "        pred = P.argmax(axis=1)\n",
        "        acc = accuracy_score(y_val, pred)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_w = w\n",
        "\n",
        "    print(\"\\nBest ensemble weights (wordSVM, charSVM, wordLR):\", best_w)\n",
        "    print(\"Validation accuracy (ensemble):\", best_acc)\n",
        "\n",
        "    # 6) FINAL fit on full training, with *CV calibration* (still much less total work now)\n",
        "    #    We refit vectorizers on full X_train for best test performance.\n",
        "    print(\"\\nRefitting final models on full training set...\")\n",
        "\n",
        "    # Word SVM final\n",
        "    final_vec_word = _build_vectorizer(best_word_cfg, max_features=max_features_word)\n",
        "    Xtr_word = final_vec_word.fit_transform(X_train)\n",
        "    Xte_word = final_vec_word.transform(X_test)\n",
        "    final_word_svm = LinearSVC(C=best_word_C, dual=\"auto\", max_iter=6000, random_state=random_state)\n",
        "\n",
        "    # cv=3 is a good speed/quality tradeoff; ensemble=False usually reduces overhead a bit\n",
        "    final_cal_word = make_calibrated(final_word_svm, cv=3, method=\"sigmoid\", ensemble=False)\n",
        "    final_cal_word.fit(Xtr_word, y_train)\n",
        "    P_word_test = final_cal_word.predict_proba(Xte_word)\n",
        "\n",
        "    # Char SVM final\n",
        "    final_vec_char = _build_vectorizer(best_char_cfg, max_features=max_features_char)\n",
        "    Xtr_char = final_vec_char.fit_transform(X_train)\n",
        "    Xte_char = final_vec_char.transform(X_test)\n",
        "    final_char_svm = LinearSVC(C=best_char_C, dual=\"auto\", max_iter=6000, random_state=random_state)\n",
        "    final_cal_char = make_calibrated(final_char_svm, cv=3, method=\"sigmoid\", ensemble=False)\n",
        "    final_cal_char.fit(Xtr_char, y_train)\n",
        "    P_char_test = final_cal_char.predict_proba(Xte_char)\n",
        "\n",
        "    # Word LR final (GPU optional)\n",
        "    final_vec_lr = _build_vectorizer(best_lr_cfg, max_features=max_features_word)\n",
        "    Xtr_lr = final_vec_lr.fit_transform(X_train)\n",
        "    Xte_lr = final_vec_lr.transform(X_test)\n",
        "\n",
        "    P_lr_test = _maybe_gpu_logreg_proba(Xtr_lr, y_train, Xte_lr, C=best_lr_C, max_iter=3000, tol=1e-3) if prefer_gpu_for_lr else None\n",
        "    if P_lr_test is None:\n",
        "        final_lr = LogisticRegression(\n",
        "            solver=\"saga\",\n",
        "            penalty=\"l2\",\n",
        "            C=best_lr_C,\n",
        "            max_iter=3000,\n",
        "            tol=1e-3,\n",
        "            n_jobs=-1,\n",
        "            multi_class=\"multinomial\",\n",
        "            random_state=random_state,\n",
        "        )\n",
        "        final_lr.fit(Xtr_lr, y_train)\n",
        "        P_lr_test = final_lr.predict_proba(Xte_lr)\n",
        "\n",
        "    # 7) Test evaluation\n",
        "    P_ens_test = weighted_vote_proba([P_word_test, P_char_test, P_lr_test], best_w)\n",
        "    test_pred = P_ens_test.argmax(axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_test, test_pred)\n",
        "    f1m = f1_score(y_test, test_pred, average=\"macro\")\n",
        "\n",
        "    print(\"\\nTEST RESULTS\")\n",
        "    print(\"Accuracy :\", acc)\n",
        "    print(\"Macro F1 :\", f1m)\n",
        "    print(\"\\nClassification report:\\n\")\n",
        "    print(classification_report(y_test, test_pred, target_names=data_test.target_names))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(random_state=21, prefer_gpu_for_lr=True)"
      ]
    }
  ]
}